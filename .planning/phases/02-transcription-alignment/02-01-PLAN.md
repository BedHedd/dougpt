---
phase: 02-transcription-alignment
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/transcription/__init__.py
  - src/transcription/transcriber.py
  - src/transcription/output.py
  - src/transcription/models.py
  - pyproject.toml
autonomous: true
requirements:
  - ALGN-01
user_setup:
  - service: whisper-models
    why: "Local Whisper models for transcription (downloaded on first use)"
    env_vars: []
    dashboard_config: []
  - service: cuda-gpu
    why: "GPU acceleration for WhisperX transcription"
    env_vars: []
    dashboard_config: []

must_haves:
  truths:
    - "User can run local transcription on a VOD and obtain word-level timestamped text"
    - "Transcript output includes both word-level and segment-level timestamps"
    - "Output exists in both JSON format (for pipelines) and SRT format (for human review)"
    - "Metadata sidecar file contains model version, language, confidence, and config info"
    - "Single transcript file per VOD (not chunked)"
  artifacts:
    - path: "src/transcription/transcriber.py"
      provides: "WhisperX/faster-whisper transcription wrapper"
      exports: ["transcribe_vod", "TranscriptionResult"]
      min_lines: 50
    - path: "src/transcription/output.py"
      provides: "JSON, SRT, and metadata output formatters"
      exports: ["write_transcript_json", "write_transcript_srt", "write_metadata"]
    - path: "src/transcription/models.py"
      provides: "Typed models for transcript data"
      exports: ["TranscriptSegment", "WordTimestamp", "TranscriptMetadata"]
  key_links:
    - from: "src/transcription/transcriber.py"
      to: "whisperx library"
      via: "import whisperx"
      pattern: "import whisperx"
    - from: "src/transcription/transcriber.py"
      to: "VOD audio file"
      via: "whisperx.load_audio()"
      pattern: "load_audio"
---

<objective>
Build local Whisper transcription pipeline with word-level timestamps and multi-format output.

Purpose: Enable local transcription of VOD audio with accurate word-level timestamps (ALGN-01), producing structured outputs suitable for downstream alignment and human review.

Output: Transcription module producing JSON transcripts, SRT subtitles, and metadata sidecars.
</objective>

<execution_context>
@/home/bedhedd/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/bedhedd/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-transcription-alignment/02-CONTEXT.md
@.planning/phases/02-transcription-alignment/02-RESEARCH.md

# Codebase context
@.planning/codebase/STACK.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/CONVENTIONS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create transcription data models</name>
  <files>src/transcription/__init__.py, src/transcription/models.py</files>
  <action>
Create typed data models for transcript data using pydantic (already in codebase).

In `src/transcription/models.py`:
- `WordTimestamp`: word (str), start (float), end (float), probability (float | None)
- `TranscriptSegment`: id (int), start (float), end (float), text (str), words (list[WordTimestamp]), avg_logprob (float | None)
- `TranscriptResult`: segments (list[TranscriptSegment]), language (str), language_probability (float), audio_path (str)
- `TranscriptMetadata`: source_file, language, model, align_model, word_count, duration_seconds, compute_type, batch_size

In `src/transcription/__init__.py`:
- Export public API: TranscriptResult, TranscriptSegment, WordTimestamp, TranscriptMetadata

Use pydantic BaseModel with field descriptions. Follow existing pydantic patterns from the codebase (see chat-extraction.py for reference).
  </action>
  <verify>
`python -c "from src.transcription import TranscriptResult, TranscriptSegment, WordTimestamp, TranscriptMetadata"` succeeds without error
  </verify>
  <done>
Models importable and instantiate correctly. Pydantic validation works on sample data.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create WhisperX transcription wrapper</name>
  <files>src/transcription/transcriber.py</files>
  <action>
Create the core transcription function wrapping WhisperX with faster-whisper backend.

Per CONTEXT.md locked decisions:
- Output BOTH word-level AND segment-level timestamps
- Single transcript file per VOD (not chunked)
- Auto-detect language

Per RESEARCH.md recommendations:
- Use WhisperX for transcription + wav2vec2 forced alignment for word-level timestamps
- Support GPU (cuda) and CPU fallback
- Handle GPU memory via configurable batch_size and compute_type

Function signature:
```python
def transcribe_vod(
    audio_path: str | Path,
    model_size: str = "large-v3",
    device: str = "cuda",
    compute_type: str = "float16",
    batch_size: int = 16,
) -> TranscriptResult:
```

Implementation:
1. Load audio with `whisperx.load_audio(audio_path)`
2. Load model with `whisperx.load_model(model_size, device, compute_type=compute_type)`
3. Transcribe with `model.transcribe(audio, batch_size=batch_size)`
4. Load align model with `whisperx.load_align_model(language_code=result["language"], device=device)`
5. Align with `whisperx.align(result["segments"], align_model, align_metadata, audio, device)`
6. Convert to typed TranscriptResult

Handle common pitfalls from RESEARCH.md:
- Set `vad_options={"vad_onset": 0.500, "vad_offset": 0.363}` to reduce hallucinations
- Use `condition_on_previous_text=False` to prevent repetition loops
- Catch CUDA OOM and provide helpful error message suggesting smaller batch_size or int8 compute_type
  </action>
  <verify>
```bash
python -c "
from src.transcription.transcriber import transcribe_vod
# Check function exists and signature is correct
import inspect
sig = inspect.signature(transcribe_vod)
assert 'audio_path' in sig.parameters
assert 'model_size' in sig.parameters
print('Signature OK')
"
```
  </verify>
  <done>
transcribe_vod function exists, accepts audio path and configuration, returns TranscriptResult with word-level timestamps.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create output formatters</name>
  <files>src/transcription/output.py</files>
  <action>
Create output formatters for JSON, SRT, and metadata files.

Per CONTEXT.md locked decisions:
- JSON format for pipelines
- SRT format for human review  
- Metadata in SEPARATE sidecar file (not inline)

Functions to implement:

```python
def write_transcript_json(result: TranscriptResult, output_path: Path) -> Path:
    """Write full transcript to JSON file."""
    # Include all segments with word-level timestamps
    # Use ensure_ascii=False for proper unicode

def write_transcript_srt(result: TranscriptResult, output_path: Path) -> Path:
    """Write transcript to SRT format for human review."""
    # Standard SRT format: index, timestamp range, text
    # Timestamp format: HH:MM:SS,mmm --> HH:MM:SS,mmm

def write_metadata(result: TranscriptResult, output_path: Path, model_size: str, compute_type: str, batch_size: int) -> Path:
    """Write metadata sidecar JSON."""
    # Include: source_file, language, model, align_model, word_count, 
    #          duration_seconds, compute_type, batch_size
```

Also create a convenience function:
```python
def write_all_outputs(
    result: TranscriptResult,
    output_dir: Path,
    base_name: str | None = None,
    model_size: str = "large-v3",
    compute_type: str = "float16", 
    batch_size: int = 16,
) -> tuple[Path, Path, Path]:
    """Write JSON, SRT, and metadata files. Returns (json_path, srt_path, metadata_path)."""
```
  </action>
  <verify>
```bash
python -c "
from src.transcription.output import write_transcript_json, write_transcript_srt, write_metadata, write_all_outputs
from src.transcription import TranscriptResult, TranscriptSegment, WordTimestamp
from pathlib import Path
import tempfile

# Create sample result
result = TranscriptResult(
    segments=[TranscriptSegment(id=1, start=0.0, end=2.0, text='Hello world', words=[WordTimestamp(word='Hello', start=0.0, end=0.5, probability=0.9)])],
    language='en',
    language_probability=0.99,
    audio_path='test.mp3'
)

with tempfile.TemporaryDirectory() as tmpdir:
    write_all_outputs(result, Path(tmpdir), 'test')
    print('Output formatters work')
"
```
  </verify>
  <done>
JSON, SRT, and metadata files can be written. SRT timestamps formatted correctly. Metadata contains all required fields.
  </done>
</task>

</tasks>

<verification>
1. All models importable from src.transcription
2. transcribe_vod function signature correct
3. Output formatters produce valid JSON, SRT, and metadata files
4. Follow codebase conventions (snake_case, pydantic models, pathlib for paths)
</verification>

<success_criteria>
- [ ] `from src.transcription import TranscriptResult, transcribe_vod` succeeds
- [ ] transcribe_vod accepts audio path and returns TranscriptResult with word-level timestamps
- [ ] write_all_outputs produces .json, .srt, and .metadata.json files
- [ ] SRT file has valid timestamp format (HH:MM:SS,mmm)
- [ ] Metadata file contains model, language, word_count, duration_seconds
- [ ] Dependencies (whisperx, faster-whisper) added to pyproject.toml
</success_criteria>

<output>
After completion, create `.planning/phases/02-transcription-alignment/02-01-SUMMARY.md`
</output>
