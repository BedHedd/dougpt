# 2026-01-25
## overall progress
- [ ] revisiting key frame chunking

## revisiting keyframe chunking (again)
After hitting a roadblock with the vision completions taking a long time, I decided to revisit the key frame chunking. It performed well reducing the total frames to 15k `15399`. but when I was passing the frames to the language model, I noticed there was too much overlap

<blockquote>
Keyframes are the wrong “unit” for this. Make the selection depend on **how far the chat actually moved**, and (optionally) **where a specific line is**.

You already have: first frame → last frame, with 2 intermediates. The “smart” thing is to pick **the best frame in that window** based on scroll distance / anchor position, and drop the rest.

## Option A: Keep only when cumulative scroll ≥ K lines (no OCR)

1. Estimate `dy` (vertical scroll) between frames cheaply.
2. Accumulate until you’ve scrolled ~`K * line_height`, then keep that frame.

```python
from __future__ import annotations

from PIL import Image, ImageOps
import numpy as np

def _row_energy(img: Image.Image) -> np.ndarray:
    g = ImageOps.grayscale(img)
    a = np.asarray(g, dtype=np.float32) / 255.0
    gx = np.zeros_like(a); gy = np.zeros_like(a)
    gx[:, 1:-1] = a[:, 2:] - a[:, :-2]
    gy[1:-1, :] = a[2:, :] - a[:-2, :]
    mag = np.hypot(gx, gy)
    row = mag.mean(axis=1)
    row = (row - row.mean()) / (row.std() + 1e-6)
    return row

def estimate_dy(a: Image.Image, b: Image.Image, max_shift: int = 200) -> tuple[int, float]:
    ra = _row_energy(a); rb = _row_energy(b)
    best_s, best_corr = 0, -1e9
    for s in range(-max_shift, max_shift + 1):
        if s >= 0:
            x = ra[s:]; y = rb[:len(x)]
        else:
            x = ra[:len(ra) + s]; y = rb[-s:]
        if len(x) < 120:
            continue
        corr = float((x * y).mean())
        if corr > best_corr:
            best_corr, best_s = corr, s
    return best_s, best_corr

def estimate_line_height(img: Image.Image) -> int:
    r = _row_energy(img)
    thr = float(np.quantile(r, 0.85))
    peaks = [i for i in range(2, len(r)-2) if r[i] > thr and r[i] > r[i-1] and r[i] > r[i+1]]
    if len(peaks) < 6:
        return 36  # fallback for Twitch chat overlays
    d = np.diff(peaks)
    d = d[(d >= 18) & (d <= 70)]
    return int(np.median(d)) if len(d) else 36

def keep_by_scroll(frames: list[Image.Image], *, lines_per_keep: int = 6, min_corr: float = 0.75) -> list[int]:
    """
    Returns indices of frames to keep.
    Keeps a frame when cumulative scroll since last kept >= lines_per_keep * line_height.
    """
    if not frames:
        return []
    lh = estimate_line_height(frames[0])
    keep = [0]
    last = 0
    accum = 0

    for i in range(1, len(frames)):
        dy, corr = estimate_dy(frames[last], frames[i])
        if corr < min_corr:
            continue
        accum += abs(dy)
        if accum >= lines_per_keep * lh:
            keep.append(i)
            last = i
            accum = 0
    return keep
```

**How to use it with your 4-frame window:** load those 4 crops into `frames` and run `keep_by_scroll(frames, lines_per_keep=4..8)`. With your sample, first→last is ~4–5 lines of scroll, so `lines_per_keep=4` will likely keep the last and drop the two in-between.

---

## Option B: Track a specific message and pick the “best” frame (still no OCR)

If your goal is “capture when line X reaches the top marker”, do **template tracking**:

* On the first frame where that message is clearly visible, crop a small template around it (username+text/emote area).
* For each candidate frame in the window, find the template’s best `y`.
* Pick the frame where `y` is closest to your target (e.g., `y_target=40` px from top of crop).

```python
def match_template_y(frame: Image.Image, templ: Image.Image, *, y0: int = 0, y1: int | None = None, step: int = 1) -> tuple[int, float]:
    """
    Slide templ vertically over frame (x fixed by templ crop) and return (best_y, score).
    Uses normalized correlation on grayscale.
    """
    F = np.asarray(ImageOps.grayscale(frame), dtype=np.float32)
    T = np.asarray(ImageOps.grayscale(templ), dtype=np.float32)
    th, tw = T.shape
    if y1 is None:
        y1 = F.shape[0] - th
    y1 = min(y1, F.shape[0] - th)
    y0 = max(0, y0)

    Tm = T - T.mean()
    Td = np.sqrt((Tm * Tm).sum()) + 1e-6

    best_y, best = y0, -1e9
    for y in range(y0, y1 + 1, step):
        P = F[y:y+th, :tw]
        Pm = P - P.mean()
        score = float((Pm * Tm).sum() / ((np.sqrt((Pm*Pm).sum()) + 1e-6) * Td))
        if score > best:
            best, best_y = score, y
    return best_y, best

def pick_frame_near_y(frames: list[Image.Image], templ: Image.Image, *, y_target: int, search_pad: int = 120) -> int:
    """
    Returns the index of the frame where the template's y is closest to y_target.
    Uses coarse dy prediction to narrow search.
    """
    # start with full search on first comparison
    y_prev, _ = match_template_y(frames[0], templ)
    best_i, best_err = 0, abs(y_prev - y_target)

    for i in range(1, len(frames)):
        # predict y window using dy estimate (fast)
        dy, corr = estimate_dy(frames[i-1], frames[i])
        y_pred = y_prev - dy  # dy sign depends; this is usually correct for upward scroll
        y0 = int(max(0, y_pred - search_pad))
        y1 = int(min(frames[i].size[1] - templ.size[1], y_pred + search_pad))

        y, score = match_template_y(frames[i], templ, y0=y0, y1=y1, step=1)
        y_prev = y

        err = abs(y - y_target)
        if err < best_err and score > 0.6:
            best_err, best_i = err, i

    return best_i
```

**Workflow for your “first/last + 2 intermediates” case**

1. Choose the message you care about (the one you previously said you want to reach the top).
2. Crop a template once (same x-range every time; include emote if it’s distinctive).
3. Run `pick_frame_near_y(frames_window, templ, y_target=some_px)` to select which of the 4 frames is the “best” capture frame.

---

## What to set for your overlay

* `line_height`: your sample crop estimates around ~35–40 px.
* Good defaults:

  * `lines_per_keep = 5–7` (cuts overlap hard while still safe)
  * `y_target = 40–80` px (near top but not clipped)

If you want, paste your crop coordinates `(x, y, w, h)` and which line you’re tracking (top boundary or yellow marker y), and I’ll give exact `y_target`, `lines_per_keep`, and the template x-range that avoids badges/icons.

</blockquote>

I asked it to pick the option that would work at scale

<blockquote>
Option A (scroll-distance gating) is the one that scales. It’s single-pass, OCR-free, and its cost per frame is small and predictable. Option B (template/anchor tracking) only makes sense when you have a specific line you want to “hit,” not for bulk downsampling.

## Recommended scalable pipeline (2-stage)

### Stage 1: Keep frames when chat has actually advanced (Option A)

Process your 20k frames in order:

1. Compute `line_height` once from an early frame.
2. For each new frame, estimate vertical shift `dy` from the last kept frame (row-energy correlation).
3. Accumulate `abs(dy)`.
4. Keep a frame when either:

   * `accum_dy >= K * line_height` (K = 6–10 is typical), or
   * bottom band changed a lot (new messages/emote-only lines arrived).

This typically cuts 20k down to a few thousand or less, depending on scroll speed and K.

**Knobs**

* `K` (lines per capture): higher = fewer frames.
* `bottom_change_thr`: prevents missing emote-only / short lines even if dy is small.

### Stage 2 (optional): Deduplicate “near-identical” kept frames

After Stage 1, you can run a cheap hash (dHash/pHash) on the kept frames and drop near-duplicates. This is often another 10–30% reduction.

## Practical defaults

* `lines_per_keep (K)`: start at **8**
* `max_shift`: **200** px (enough for several lines + jitter)
* `min_corr`: **0.75**
* `bottom_band_h`: **120–160** px
* `bottom_change_thr`: **0.04–0.06** (depends on compression)

## Speed improvements (important at 20k+)

* Compute the row-energy on a **downscaled** version of the crop:

  * resize width to ~200–300 px (keep height), or even downscale both by 2
* Use only the **chat text columns** (exclude left badge/icon column and any right border) before computing row-energy; it stabilizes correlation.
* Process in chunks, but keep the last kept frame from the previous chunk to seed the next.

## When to use Option B

Only after Stage 1, for targeted tasks like: “pick the frame where message X hits y=40px.” You’d run template tracking on the small window around that event, not across 20k frames.

If you want, paste:

* crop box `(x0, y0, x1, y1)` (or your current crop image width/height),
* and your current average candidate rate (e.g., “20k frames from keyframes over 4h”),
  and I’ll give you a concrete K that should land you near a target count (e.g., 2k–5k).
</blockquote>

I then told it to give a function to process it and tested it with a 30 second chunk. Some cycles later this was the response that worked

<blockquote>
Your report shows why it kept everything:

* `kept_count: 100799` equals the number of decoded frames (60 fps decode, no sampling).
* `bottom_change` is ~0.08–0.14 almost every time, but you used `bottom_change_thr: 0.05`, so the “bottom changed” rule fires constantly and forces a keep.
* That’s why you see adjacent keeps (424 → 429 → 457) even though overlap is high.

### Fix 1: Raise/disable bottom-change forcing

Given your observed `bottom_change` distribution, set `bottom_change_thr` higher than typical noise.

Start here:

```python
kept = reduce_chat_frames_by_scroll(
    ai_invasion_1_cropped,
    out_dir,
    lines_per_keep=8,
    bottom_change_thr=0.14,   # was 0.05
)
```

If you don’t need bottom-change at all, effectively disable it:

```python
kept = reduce_chat_frames_by_scroll(
    ai_invasion_1_cropped,
    out_dir,
    lines_per_keep=8,
    bottom_change_thr=1.0,    # never triggers
)
```

### Fix 2: Accumulate scroll frame-to-frame (not vs last kept)

If you ran the earlier version, it was comparing the current frame to the last kept frame and then adding `abs(dy)` repeatedly, which is not the correct “distance traveled so far.” Accumulate `dy_step` between consecutive frames.

Use the corrected function version (the one with `dy_step_px` and `accum_scroll` updated from `prev_frame -> current_frame`). If you’re not sure which version you executed: your report fields include `dy_px` (older version), not `dy_step_px` (corrected version).

### Fix 3: Don’t decode all 60-fps frames unless you want to

If your keyframe method gives ~15k candidates and you want fewer, you should either:

#### A) Decode only keyframes with ffmpeg (best drop-in)

Modify the ffmpeg reader to add `-skip_frame nokey`:

```python
def _iter_gray_frames_ffmpeg(video: Path, w: int, h: int, *, sample_fps=None, keyframes_only: bool = False):
    vf = "format=gray"
    if sample_fps is not None:
        vf = f"fps={sample_fps},{vf}"

    cmd = ["ffmpeg", "-hide_banner", "-loglevel", "error", "-vsync", "0"]
    if keyframes_only:
        cmd += ["-skip_frame", "nokey"]
    cmd += ["-i", str(video), "-an", "-vf", vf, "-f", "rawvideo", "-pix_fmt", "gray", "pipe:1"]
    ...
```

Then call:

```python
kept = reduce_chat_frames_by_scroll(
    ai_invasion_1_cropped,
    out_dir,
    keyframes_only=True,   # add this param and thread it through
    lines_per_keep=8,
    bottom_change_thr=0.14,
)
```

This keeps the “candidate set” comparable to your keyframe extraction, then applies scroll-gating to remove overlap.

#### B) Sample FPS (simple)

If you want a cheap reduction before gating:

```python
kept = reduce_chat_frames_by_scroll(
    ai_invasion_1_cropped,
    out_dir,
    sample_fps=3.0,
    lines_per_keep=8,
    bottom_change_thr=0.14,
)
```

### Quick sanity check after changes

After a short run, `kept_count` should be **much smaller** than decoded frames, and you should stop seeing near-adjacent indices being kept unless chat is genuinely static.

If you paste the first ~30 `bottom_change` values from the report, I can give a threshold that cleanly separates “noise” from “new message arrival” for your encode.
</blockquote>

Some cycles back and forth this was the version that worked

<blockquote>
Yes.

### Reduce further (3 knobs)

1. Increase `lines_per_keep` (e.g. 12–18)
2. Set `sample_fps` (e.g. 2–4) so you don’t decode 60 fps
3. Add a minimum time between kept frames (`min_keep_interval_sec`) to prevent rapid keeps in bursts

### Final version that saves color PNGs

This decodes **RGB**, computes alignment on **luma** (grayscale derived from RGB), and saves **color** frames.

```python
from __future__ import annotations

from dataclasses import dataclass, asdict
from pathlib import Path
import json
import subprocess
from typing import Iterable, Optional

import numpy as np
from PIL import Image


@dataclass(frozen=True)
class KeptFrame:
    index: int
    t_sec: float
    dy_step_px: int
    corr: float
    bottom_change: float
    saved_path: Optional[str]


def _ffprobe_video_info(video: Path) -> tuple[int, int, float]:
    cmd = [
        "ffprobe", "-v", "error",
        "-select_streams", "v:0",
        "-show_entries", "stream=width,height,avg_frame_rate,r_frame_rate",
        "-of", "json",
        str(video),
    ]
    out = subprocess.check_output(cmd)
    info = json.loads(out.decode("utf-8"))
    s = info["streams"][0]
    w, h = int(s["width"]), int(s["height"])

    def parse_rate(x: str) -> float:
        if not x or x == "0/0":
            return 0.0
        n, d = x.split("/")
        n = float(n)
        d = float(d) if float(d) != 0 else 1.0
        return n / d

    fps = parse_rate(s.get("avg_frame_rate", "")) or parse_rate(s.get("r_frame_rate", "")) or 1.0
    return w, h, fps


def _iter_rgb_frames_ffmpeg(
    video: Path,
    w: int,
    h: int,
    *,
    sample_fps: Optional[float] = None,
    start_sec: Optional[float] = None,
    duration_sec: Optional[float] = None,
    keyframes_only: bool = False,
) -> Iterable[np.ndarray]:
    cmd = ["ffmpeg", "-hide_banner", "-loglevel", "error", "-vsync", "0"]

    if start_sec is not None:
        cmd += ["-ss", str(float(start_sec))]
    if duration_sec is not None:
        cmd += ["-t", str(float(duration_sec))]
    if keyframes_only:
        cmd += ["-skip_frame", "nokey"]

    cmd += ["-i", str(video), "-an"]

    if sample_fps is not None:
        cmd += ["-vf", f"fps={sample_fps}"]

    cmd += ["-f", "rawvideo", "-pix_fmt", "rgb24", "pipe:1"]

    frame_size = w * h * 3
    with subprocess.Popen(cmd, stdout=subprocess.PIPE) as proc:
        assert proc.stdout is not None
        while True:
            buf = proc.stdout.read(frame_size)
            if not buf or len(buf) < frame_size:
                break
            rgb = np.frombuffer(buf, dtype=np.uint8).reshape((h, w, 3))
            yield rgb
        proc.wait()


def _luma_u8(rgb: np.ndarray) -> np.ndarray:
    # ITU-R BT.601 luma approximation
    r = rgb[..., 0].astype(np.float32)
    g = rgb[..., 1].astype(np.float32)
    b = rgb[..., 2].astype(np.float32)
    y = 0.299 * r + 0.587 * g + 0.114 * b
    return np.clip(y, 0, 255).astype(np.uint8)


def _row_energy(gray_u8: np.ndarray, downsample: int = 2) -> np.ndarray:
    a = gray_u8[::downsample, ::downsample].astype(np.float32) / 255.0
    dx = np.abs(a[:, 1:] - a[:, :-1]).mean(axis=1)
    dy = np.abs(a[1:, :] - a[:-1, :]).mean(axis=1)
    dy = np.concatenate([dy, dy[-1:]])
    r = dx + dy
    return (r - r.mean()) / (r.std() + 1e-6)


def _estimate_dy(row_a: np.ndarray, row_b: np.ndarray, max_shift: int) -> tuple[int, float]:
    best_s, best_corr = 0, -1e9
    n = len(row_a)
    for s in range(-max_shift, max_shift + 1):
        if s >= 0:
            x = row_a[s:]
            y = row_b[: len(x)]
        else:
            x = row_a[: n + s]
            y = row_b[-s:]
        if len(x) < 80:
            continue
        corr = float((x * y).mean())
        if corr > best_corr:
            best_corr, best_s = corr, s
    return best_s, best_corr


def _estimate_line_height(row: np.ndarray) -> int:
    thr = float(np.quantile(row, 0.85))
    peaks = [i for i in range(2, len(row) - 2) if row[i] > thr and row[i] > row[i - 1] and row[i] > row[i + 1]]
    if len(peaks) < 6:
        return 18  # downsampled fallback (~36px if downsample=2)
    d = np.diff(peaks)
    d = d[(d >= 9) & (d <= 35)]
    return int(np.median(d)) if len(d) else 18


def _bottom_change(gray_a: np.ndarray, gray_b: np.ndarray, band_h: int, downsample: int = 2) -> float:
    a = gray_a[::downsample, ::downsample].astype(np.float32) / 255.0
    b = gray_b[::downsample, ::downsample].astype(np.float32) / 255.0
    bh = max(1, band_h // downsample)
    return float(np.mean(np.abs(a[-bh:, :] - b[-bh:, :])))


def reduce_chat_frames_by_scroll_color(
    video_path: Path | str,
    out_dir: Path | str,
    *,
    start_sec: float | None = None,
    duration_sec: float | None = None,
    sample_fps: Optional[float] = None,
    keyframes_only: bool = False,
    lines_per_keep: int = 12,
    min_corr: float = 0.75,
    max_shift_px: int = 220,
    bottom_band_h_px: int = 160,
    bottom_change_thr: float = 0.16,
    min_keep_interval_sec: float = 0.8,     # additional reduction lever
    burst_override_mult: float = 2.0,       # allow keep sooner if scroll is huge
    downsample: int = 2,
    save_png: bool = True,
    write_report_json: bool = True,
    progress_every: int = 1000,
) -> list[KeptFrame]:
    video = Path(video_path)
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)

    w, h, fps_native = _ffprobe_video_info(video)
    fps_used = float(sample_fps) if sample_fps is not None else float(fps_native)
    t0 = float(start_sec or 0.0)

    max_shift_ds = max(10, int(max_shift_px / downsample))

    kept: list[KeptFrame] = []
    prev_gray: Optional[np.ndarray] = None
    prev_row: Optional[np.ndarray] = None
    line_h_ds: Optional[int] = None

    accum_scroll = 0.0
    last_keep_t = -1e9

    for i, rgb in enumerate(
        _iter_rgb_frames_ffmpeg(
            video, w, h,
            sample_fps=sample_fps,
            start_sec=start_sec,
            duration_sec=duration_sec,
            keyframes_only=keyframes_only,
        )
    ):
        if progress_every and (i % progress_every == 0) and i > 0:
            print(f"[reduce] decoded={i} kept={len(kept)}")

        t_sec = t0 + (i / fps_used)
        gray = _luma_u8(rgb)
        row = _row_energy(gray, downsample=downsample)

        if prev_gray is None:
            line_h_ds = _estimate_line_height(row)
            save_path = None
            if save_png:
                save_path = str(out / f"frame_{i:06d}_t{t_sec:010.3f}.png")
                Image.fromarray(rgb, mode="RGB").save(save_path)
            kept.append(KeptFrame(i, float(t_sec), 0, 1.0, 0.0, save_path))
            prev_gray, prev_row = gray, row
            last_keep_t = t_sec
            continue

        assert prev_row is not None and prev_gray is not None and line_h_ds is not None

        dy_ds, corr = _estimate_dy(prev_row, row, max_shift=max_shift_ds)
        if corr >= min_corr:
            accum_scroll += abs(dy_ds)
        else:
            dy_ds = 0

        bc = _bottom_change(prev_gray, gray, band_h=bottom_band_h_px, downsample=downsample)

        # bottom-change only forces a keep when there's little/no scroll
        force_new = (abs(dy_ds) < 0.5 * line_h_ds) and (bc >= bottom_change_thr)

        scroll_trigger = accum_scroll >= (lines_per_keep * line_h_ds)
        burst_override = accum_scroll >= (burst_override_mult * lines_per_keep * line_h_ds)

        interval_ok = (t_sec - last_keep_t) >= min_keep_interval_sec
        keep_now = ((scroll_trigger and interval_ok) or burst_override) or (force_new and interval_ok)

        if keep_now:
            save_path = None
            if save_png:
                save_path = str(out / f"frame_{i:06d}_t{t_sec:010.3f}.png")
                Image.fromarray(rgb, mode="RGB").save(save_path)
            kept.append(KeptFrame(i, float(t_sec), int(dy_ds * downsample), float(corr), float(bc), save_path))
            accum_scroll = 0.0
            last_keep_t = t_sec

        prev_gray, prev_row = gray, row

    if write_report_json:
        report = {
            "video": str(video),
            "start_sec": start_sec,
            "duration_sec": duration_sec,
            "sample_fps": sample_fps,
            "keyframes_only": keyframes_only,
            "fps_used_for_timestamps": fps_used,
            "lines_per_keep": lines_per_keep,
            "min_keep_interval_sec": min_keep_interval_sec,
            "min_corr": min_corr,
            "max_shift_px": max_shift_px,
            "bottom_band_h_px": bottom_band_h_px,
            "bottom_change_thr": bottom_change_thr,
            "downsample": downsample,
            "kept_count": len(kept),
            "kept": [asdict(k) for k in kept],
        }
        (out / "report.json").write_text(json.dumps(report, indent=2), encoding="utf-8")

    return kept
```

### Suggested test call (first 30s, fewer frames)

```python
out_dir = data_dir / "chat_frames_test_30s_color"
kept = reduce_chat_frames_by_scroll_color(
    ai_invasion_1_cropped,
    out_dir,
    start_sec=0,
    duration_sec=30,
    sample_fps=3.0,          # big reduction vs 60fps decode
    lines_per_keep=14,       # increase to reduce further
    min_keep_interval_sec=1.0,
    bottom_change_thr=0.18,  # raise if it still keeps too often
    progress_every=500,
)
```

If it’s still keeping too many:

* bump `lines_per_keep` to **16–20**
* bump `min_keep_interval_sec` to **1.2–1.5**
* bump `bottom_change_thr` to **0.20–0.25** (your encode noise can be high)

Once it’s dialed in on 30s, remove `duration_sec` and run the full video.

</blockquote>

After inspecting the 30s outputs, I ran it on the full stream. It took 20 minutes to run on the full 4hr stream, and the final frames count was `6543` (`15399`-> `6543`), Impressive! 