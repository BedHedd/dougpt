{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Extraction Review Pipeline\n",
    "\n",
    "Notebook-first local pipeline for:\n",
    "\n",
    "1. Single-file or batch media input discovery\n",
    "2. Deterministic ffmpeg extraction to mono 16k FLAC\n",
    "3. Local `faster-whisper` transcription with reusable transcript JSON artifacts\n",
    "4. Optional best-effort diarization via WhisperX/pyannote with `UNKNOWN` fallback\n",
    "5. Per-run metadata snapshots with stage timings and resumable checkpoints\n",
    "6. Schema-constrained local LLM segmentation into chat-style blocks with chronology/overlap validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime, timezone\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except Exception:\n",
    "    def load_dotenv(*_args: Any, **_kwargs: Any) -> bool:\n",
    "        return False\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from uuid import uuid4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: dict[str, Any] = {\n",
    "    \"input_mode\": \"single\",  # \"single\" | \"batch\"\n",
    "    \"single_input\": \"large-files/Doug and Twitch Chat TAKE OVER EUROPE-VpmmuHlLPM0.mkv\",\n",
    "    \"batch_inputs\": [],\n",
    "    \"batch_glob\": \"*.mkv\",\n",
    "    \"force_reextract\": False,\n",
    "    \"force_retranscribe\": False,\n",
    "    \"pipeline\": {\n",
    "        \"allow_transcript_checkpoint_reuse\": True,\n",
    "        \"reuse_transcript_glob\": \"*.json\",\n",
    "    },\n",
    "    \"ffmpeg\": {\n",
    "        \"audio_codec\": \"flac\",\n",
    "        \"sample_rate\": 16000,\n",
    "        \"channels\": 1,\n",
    "        \"overwrite\": True,\n",
    "    },\n",
    "    \"transcription\": {\n",
    "        \"model_name\": \"tiny.en\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"compute_type\": \"int8\",\n",
    "        \"beam_size\": 5,\n",
    "        \"vad_filter\": True,\n",
    "        \"word_timestamps\": True,\n",
    "    },\n",
    "    \"diarization\": {\n",
    "        \"enabled\": True,\n",
    "        \"provider\": \"whisperx\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"min_overlap_seconds\": 0.2,\n",
    "},\n",
    "\"segmentation\": {\n",
    "    \"enabled\": True,\n",
    "    \"base_url\": \"http://localhost:1234/v1\",\n",
    "    \"api_key\": \"lm-studio\",\n",
    "    \"model\": \"qwen3-vl-8b-instruct\",\n",
    "    \"temperature\": 0.2,\n",
    "    \"window_seconds\": 240.0,\n",
    "    \"window_overlap_seconds\": 20.0,\n",
    "    \"minimum_overlap_seconds\": 1.5,\n",
    "    \"max_summary_words\": 80,\n",
    "    \"style_reference\": \"00-dev-log/2026-02-09.md\",\n",
    "    \"smoke_check_timeout_seconds\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "\n",
    "def resolve_project_paths(start: Path | None = None) -> dict[str, Path]:\n",
    "    start_path = (start or Path.cwd()).resolve()\n",
    "    candidates = [start_path, *start_path.parents]\n",
    "    anchor = next((p / \"00-supporting-files\" for p in candidates if (p / \"00-supporting-files\").exists()), None)\n",
    "    if anchor is None:\n",
    "        raise FileNotFoundError(\"Could not locate 00-supporting-files anchor from current notebook path\")\n",
    "\n",
    "    project_root = anchor.parent\n",
    "    project_parent = project_root.parent\n",
    "    media_root = project_parent / \"large-files\"\n",
    "    if not media_root.exists():\n",
    "        media_root = project_root / \"large-files\"\n",
    "\n",
    "    env_path = anchor / \"data\" / \".env\"\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path, override=False)\n",
    "\n",
    "    data_root = anchor / \"data\" / \"audio-extraction-review\"\n",
    "    paths = {\n",
    "        \"project_root\": project_root,\n",
    "        \"supporting_files\": anchor,\n",
    "        \"project_parent\": project_parent,\n",
    "        \"media_root\": media_root,\n",
    "        \"env_path\": env_path,\n",
    "        \"data_root\": data_root,\n",
    "        \"audio_dir\": data_root / \"audio\",\n",
    "        \"logs_dir\": data_root / \"logs\",\n",
    "        \"runs_dir\": data_root / \"runs\",\n",
    "        \"transcripts_dir\": data_root / \"transcripts\",\n",
    "    }\n",
    "    for key in (\"data_root\", \"audio_dir\", \"logs_dir\", \"runs_dir\", \"transcripts_dir\"):\n",
    "        paths[key].mkdir(parents=True, exist_ok=True)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def as_project_relative(path: Path, project_root: Path) -> str:\n",
    "    try:\n",
    "        return str(path.resolve().relative_to(project_root))\n",
    "    except Exception:\n",
    "        return str(path.resolve())\n",
    "\n",
    "\n",
    "def discover_inputs(config: dict[str, Any], paths: dict[str, Path]) -> list[Path]:\n",
    "    project_root = paths[\"project_root\"]\n",
    "    media_root = paths[\"media_root\"]\n",
    "    mode = config[\"input_mode\"].strip().lower()\n",
    "\n",
    "    if mode == \"single\":\n",
    "        p = Path(config[\"single_input\"])\n",
    "        if not p.is_absolute():\n",
    "            if p.parts and p.parts[0] == \"large-files\":\n",
    "                p = (media_root / Path(*p.parts[1:])).resolve()\n",
    "            else:\n",
    "                p = (project_root / p).resolve()\n",
    "        return [p]\n",
    "\n",
    "    if mode == \"batch\":\n",
    "        discovered: list[Path] = []\n",
    "        for item in config.get(\"batch_inputs\", []):\n",
    "            p = Path(item)\n",
    "            if not p.is_absolute():\n",
    "                if p.parts and p.parts[0] == \"large-files\":\n",
    "                    p = (media_root / Path(*p.parts[1:])).resolve()\n",
    "                else:\n",
    "                    p = (project_root / p).resolve()\n",
    "            discovered.append(p)\n",
    "\n",
    "        if config.get(\"batch_glob\"):\n",
    "            if media_root.exists():\n",
    "                discovered.extend(sorted(media_root.glob(config[\"batch_glob\"])))\n",
    "\n",
    "        unique = sorted({p.resolve() for p in discovered})\n",
    "        return unique\n",
    "\n",
    "    raise ValueError(f\"Unsupported input_mode={mode}; expected 'single' or 'batch'\")\n",
    "\n",
    "\n",
    "def output_audio_path(input_media: Path, audio_dir: Path) -> Path:\n",
    "    safe_stem = input_media.stem.replace(\" \", \"_\")\n",
    "    return audio_dir / f\"{safe_stem}.flac\"\n",
    "\n",
    "\n",
    "def append_jsonl(path: Path, record: dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(json.dumps(record, ensure_ascii=True) + \"\\n\")\n",
    "\n",
    "\n",
    "def ffprobe_duration_seconds(path: Path) -> float | None:\n",
    "    cmd = [\n",
    "        \"ffprobe\",\n",
    "        \"-v\",\n",
    "        \"error\",\n",
    "        \"-show_entries\",\n",
    "        \"format=duration\",\n",
    "        \"-of\",\n",
    "        \"default=noprint_wrappers=1:nokey=1\",\n",
    "        str(path),\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        return None\n",
    "    raw = result.stdout.strip()\n",
    "    if not raw:\n",
    "        return None\n",
    "    try:\n",
    "        return float(raw)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_audio(input_media: Path, output_audio: Path, ffmpeg_cfg: dict[str, Any]) -> subprocess.CompletedProcess[str]:\n",
    "    cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-v\",\n",
    "        \"error\",\n",
    "        \"-y\" if ffmpeg_cfg.get(\"overwrite\", True) else \"-n\",\n",
    "        \"-i\",\n",
    "        str(input_media),\n",
    "        \"-vn\",\n",
    "        \"-ac\",\n",
    "        str(ffmpeg_cfg.get(\"channels\", 1)),\n",
    "        \"-ar\",\n",
    "        str(ffmpeg_cfg.get(\"sample_rate\", 16000)),\n",
    "        \"-c:a\",\n",
    "        str(ffmpeg_cfg.get(\"audio_codec\", \"flac\")),\n",
    "        str(output_audio),\n",
    "    ]\n",
    "    return subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "\n",
    "def run_extraction_stage(\n",
    "    *,\n",
    "    inputs: list[Path],\n",
    "    paths: dict[str, Path],\n",
    "    config: dict[str, Any],\n",
    "    run_id: str,\n",
    ") -> dict[str, Any]:\n",
    "    stage_start = time.perf_counter()\n",
    "    extraction_log = paths[\"logs_dir\"] / f\"extraction-{run_id}.jsonl\"\n",
    "    failure_log = paths[\"logs_dir\"] / \"extraction-failures.jsonl\"\n",
    "\n",
    "    records: list[dict[str, Any]] = []\n",
    "    failures: list[dict[str, Any]] = []\n",
    "\n",
    "    for input_media in inputs:\n",
    "        record: dict[str, Any] = {\n",
    "            \"run_id\": run_id,\n",
    "            \"timestamp\": now_iso(),\n",
    "            \"stage\": \"extract\",\n",
    "            \"input_media\": str(input_media),\n",
    "            \"status\": \"pending\",\n",
    "        }\n",
    "\n",
    "        if not input_media.exists():\n",
    "            record.update({\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": \"input_not_found\",\n",
    "            })\n",
    "            append_jsonl(extraction_log, record)\n",
    "            append_jsonl(failure_log, record)\n",
    "            failures.append(record)\n",
    "            records.append(record)\n",
    "            continue\n",
    "\n",
    "        output_audio = output_audio_path(input_media, paths[\"audio_dir\"])\n",
    "        if output_audio.exists() and not config.get(\"force_reextract\", False):\n",
    "            record.update({\n",
    "                \"status\": \"reused\",\n",
    "                \"audio_path\": str(output_audio),\n",
    "                \"resume_marker\": True,\n",
    "            })\n",
    "            append_jsonl(extraction_log, record)\n",
    "            records.append(record)\n",
    "            continue\n",
    "\n",
    "        cmd_result = extract_audio(input_media, output_audio, config[\"ffmpeg\"])\n",
    "        if cmd_result.returncode != 0:\n",
    "            record.update({\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": \"ffmpeg_failed\",\n",
    "                \"stderr\": cmd_result.stderr.strip(),\n",
    "                \"command\": \" \".join(shlex.quote(part) for part in cmd_result.args),\n",
    "            })\n",
    "            append_jsonl(extraction_log, record)\n",
    "            append_jsonl(failure_log, record)\n",
    "            failures.append(record)\n",
    "            records.append(record)\n",
    "            continue\n",
    "\n",
    "        record.update({\n",
    "            \"status\": \"ok\",\n",
    "            \"audio_path\": str(output_audio),\n",
    "            \"audio_duration_seconds\": ffprobe_duration_seconds(output_audio),\n",
    "            \"resume_marker\": False,\n",
    "        })\n",
    "        append_jsonl(extraction_log, record)\n",
    "        records.append(record)\n",
    "\n",
    "    duration = time.perf_counter() - stage_start\n",
    "    return {\n",
    "        \"stage\": \"extract\",\n",
    "        \"duration_seconds\": round(duration, 3),\n",
    "        \"records\": records,\n",
    "        \"failures\": failures,\n",
    "        \"log_path\": str(extraction_log),\n",
    "        \"failure_log_path\": str(failure_log),\n",
    "    }\n",
    "\n",
    "\n",
    "def _segment_overlap_seconds(segment_start: float, segment_end: float, diar_start: float, diar_end: float) -> float:\n",
    "    overlap = min(segment_end, diar_end) - max(segment_start, diar_start)\n",
    "    return max(0.0, overlap)\n",
    "\n",
    "\n",
    "def best_effort_diarization(\n",
    "    *,\n",
    "    audio_path: Path,\n",
    "    config: dict[str, Any],\n",
    ") -> tuple[list[dict[str, Any]], dict[str, Any]]:\n",
    "    diar_cfg = config.get(\"diarization\", {})\n",
    "    if not diar_cfg.get(\"enabled\", True):\n",
    "        return [], {\n",
    "            \"attempted\": False,\n",
    "            \"provider\": diar_cfg.get(\"provider\", \"whisperx\"),\n",
    "            \"used\": False,\n",
    "            \"fallback_reason\": \"disabled_in_config\",\n",
    "        }\n",
    "\n",
    "    token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "    if not token:\n",
    "        return [], {\n",
    "            \"attempted\": True,\n",
    "            \"provider\": diar_cfg.get(\"provider\", \"whisperx\"),\n",
    "            \"used\": False,\n",
    "            \"fallback_reason\": \"missing_huggingface_token\",\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        import whisperx\n",
    "        from whisperx.diarize import DiarizationPipeline\n",
    "    except Exception:\n",
    "        return [], {\n",
    "            \"attempted\": True,\n",
    "            \"provider\": diar_cfg.get(\"provider\", \"whisperx\"),\n",
    "            \"used\": False,\n",
    "            \"fallback_reason\": \"whisperx_or_pyannote_not_installed\",\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        audio = whisperx.load_audio(str(audio_path))\n",
    "        diarize_model = DiarizationPipeline(token=token, device=diar_cfg.get(\"device\", \"cpu\"))\n",
    "        diar_df = diarize_model(audio)\n",
    "\n",
    "        diar_segments: list[dict[str, Any]] = []\n",
    "        for _, row in diar_df.iterrows():\n",
    "            diar_segments.append({\n",
    "                \"start\": float(row[\"start\"]),\n",
    "                \"end\": float(row[\"end\"]),\n",
    "                \"speaker\": str(row[\"speaker\"]),\n",
    "            })\n",
    "\n",
    "        return diar_segments, {\n",
    "            \"attempted\": True,\n",
    "            \"provider\": diar_cfg.get(\"provider\", \"whisperx\"),\n",
    "            \"used\": True,\n",
    "            \"fallback_reason\": None,\n",
    "        }\n",
    "    except Exception as exc:\n",
    "        return [], {\n",
    "            \"attempted\": True,\n",
    "            \"provider\": diar_cfg.get(\"provider\", \"whisperx\"),\n",
    "            \"used\": False,\n",
    "            \"fallback_reason\": f\"diarization_failed: {exc}\",\n",
    "        }\n",
    "\n",
    "\n",
    "def pick_segment_speaker(\n",
    "    *,\n",
    "    segment_start: float,\n",
    "    segment_end: float,\n",
    "    diar_segments: list[dict[str, Any]],\n",
    "    min_overlap_seconds: float,\n",
    ") -> str:\n",
    "    if not diar_segments:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    best: tuple[float, str] | None = None\n",
    "    for diar in diar_segments:\n",
    "        overlap = _segment_overlap_seconds(segment_start, segment_end, diar[\"start\"], diar[\"end\"])\n",
    "        if overlap <= 0:\n",
    "            continue\n",
    "        if best is None or overlap > best[0]:\n",
    "            best = (overlap, diar[\"speaker\"])\n",
    "\n",
    "    if best is None or best[0] < min_overlap_seconds:\n",
    "        return \"UNKNOWN\"\n",
    "    return best[1]\n",
    "\n",
    "\n",
    "def transcribe_audio_with_faster_whisper(audio_path: Path, config: dict[str, Any]) -> tuple[list[dict[str, Any]], dict[str, Any]]:\n",
    "    try:\n",
    "        from faster_whisper import WhisperModel\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"faster_whisper_not_installed\") from exc\n",
    "\n",
    "    t_cfg = config[\"transcription\"]\n",
    "    model = WhisperModel(\n",
    "        t_cfg.get(\"model_name\", \"tiny.en\"),\n",
    "        device=t_cfg.get(\"device\", \"cpu\"),\n",
    "        compute_type=t_cfg.get(\"compute_type\", \"int8\"),\n",
    "    )\n",
    "\n",
    "    segments_iter, info = model.transcribe(\n",
    "        str(audio_path),\n",
    "        beam_size=t_cfg.get(\"beam_size\", 5),\n",
    "        vad_filter=t_cfg.get(\"vad_filter\", True),\n",
    "        word_timestamps=t_cfg.get(\"word_timestamps\", True),\n",
    "    )\n",
    "\n",
    "    segments: list[dict[str, Any]] = []\n",
    "    for idx, seg in enumerate(segments_iter, start=1):\n",
    "        words = []\n",
    "        for w in (seg.words or []):\n",
    "            words.append({\n",
    "                \"start\": float(w.start),\n",
    "                \"end\": float(w.end),\n",
    "                \"word\": w.word,\n",
    "                \"probability\": float(w.probability),\n",
    "            })\n",
    "\n",
    "        segments.append({\n",
    "            \"id\": idx,\n",
    "            \"start\": float(seg.start),\n",
    "            \"end\": float(seg.end),\n",
    "            \"text\": seg.text.strip(),\n",
    "            \"words\": words,\n",
    "        })\n",
    "\n",
    "    info_payload = {\n",
    "        \"language\": getattr(info, \"language\", None),\n",
    "        \"language_probability\": float(getattr(info, \"language_probability\", 0.0) or 0.0),\n",
    "        \"duration\": float(getattr(info, \"duration\", 0.0) or 0.0),\n",
    "        \"duration_after_vad\": float(getattr(info, \"duration_after_vad\", 0.0) or 0.0),\n",
    "    }\n",
    "    return segments, info_payload\n",
    "\n",
    "\n",
    "def transcript_output_path(audio_path: Path, transcripts_dir: Path) -> Path:\n",
    "    return transcripts_dir / f\"{audio_path.stem}.json\"\n",
    "\n",
    "\n",
    "def run_transcription_stage(\n",
    "    *,\n",
    "    extraction_records: list[dict[str, Any]],\n",
    "    paths: dict[str, Path],\n",
    "    config: dict[str, Any],\n",
    "    run_id: str,\n",
    ") -> dict[str, Any]:\n",
    "    stage_start = time.perf_counter()\n",
    "    run_log = paths[\"logs_dir\"] / f\"transcription-{run_id}.jsonl\"\n",
    "\n",
    "    records: list[dict[str, Any]] = []\n",
    "    failures: list[dict[str, Any]] = []\n",
    "\n",
    "    for item in extraction_records:\n",
    "        if item.get(\"status\") not in {\"ok\", \"reused\"}:\n",
    "            continue\n",
    "\n",
    "        audio_path = Path(item[\"audio_path\"])  # absolute path from extraction stage\n",
    "        transcript_path = transcript_output_path(audio_path, paths[\"transcripts_dir\"])\n",
    "        record: dict[str, Any] = {\n",
    "            \"run_id\": run_id,\n",
    "            \"timestamp\": now_iso(),\n",
    "            \"stage\": \"transcribe\",\n",
    "            \"audio_path\": str(audio_path),\n",
    "            \"transcript_path\": str(transcript_path),\n",
    "            \"status\": \"pending\",\n",
    "        }\n",
    "\n",
    "        if transcript_path.exists() and not config.get(\"force_retranscribe\", False):\n",
    "            record.update({\n",
    "                \"status\": \"reused\",\n",
    "                \"resume_marker\": True,\n",
    "            })\n",
    "            append_jsonl(run_log, record)\n",
    "            records.append(record)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            segments, info_payload = transcribe_audio_with_faster_whisper(audio_path, config)\n",
    "            diar_segments, diar_meta = best_effort_diarization(audio_path=audio_path, config=config)\n",
    "\n",
    "            min_overlap = float(config.get(\"diarization\", {}).get(\"min_overlap_seconds\", 0.2))\n",
    "            normalized_segments: list[dict[str, Any]] = []\n",
    "            for seg in segments:\n",
    "                speaker = pick_segment_speaker(\n",
    "                    segment_start=seg[\"start\"],\n",
    "                    segment_end=seg[\"end\"],\n",
    "                    diar_segments=diar_segments,\n",
    "                    min_overlap_seconds=min_overlap,\n",
    "                )\n",
    "                normalized_segments.append({\n",
    "                    \"id\": seg[\"id\"],\n",
    "                    \"start\": seg[\"start\"],\n",
    "                    \"end\": seg[\"end\"],\n",
    "                    \"speaker\": speaker,\n",
    "                    \"text\": seg[\"text\"],\n",
    "                    \"words\": seg[\"words\"],\n",
    "                })\n",
    "\n",
    "            source_media = item.get(\"input_media\")\n",
    "            transcript_payload = {\n",
    "                \"schema_version\": \"1.0\",\n",
    "                \"run_id\": run_id,\n",
    "                \"created_at\": now_iso(),\n",
    "                \"source\": {\n",
    "                    \"media_path\": source_media,\n",
    "                    \"audio_path\": str(audio_path),\n",
    "                    \"audio_duration_seconds\": ffprobe_duration_seconds(audio_path),\n",
    "                },\n",
    "                \"transcription\": {\n",
    "                    \"engine\": \"faster-whisper\",\n",
    "                    \"model_name\": config[\"transcription\"].get(\"model_name\"),\n",
    "                    \"device\": config[\"transcription\"].get(\"device\"),\n",
    "                    \"compute_type\": config[\"transcription\"].get(\"compute_type\"),\n",
    "                    \"language\": info_payload.get(\"language\"),\n",
    "                    \"language_probability\": info_payload.get(\"language_probability\"),\n",
    "                    \"duration_seconds\": info_payload.get(\"duration\"),\n",
    "                    \"duration_after_vad_seconds\": info_payload.get(\"duration_after_vad\"),\n",
    "                },\n",
    "                \"diarization\": diar_meta,\n",
    "                \"segments\": normalized_segments,\n",
    "            }\n",
    "\n",
    "            transcript_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            transcript_path.write_text(json.dumps(transcript_payload, indent=2, ensure_ascii=True), encoding=\"utf-8\")\n",
    "\n",
    "            record.update({\n",
    "                \"status\": \"ok\",\n",
    "                \"segment_count\": len(normalized_segments),\n",
    "                \"word_timestamp_count\": sum(len(seg[\"words\"]) for seg in normalized_segments),\n",
    "                \"resume_marker\": False,\n",
    "                \"diarization_fallback_reason\": diar_meta.get(\"fallback_reason\"),\n",
    "            })\n",
    "            append_jsonl(run_log, record)\n",
    "            records.append(record)\n",
    "        except Exception as exc:\n",
    "            record.update({\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(exc),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "            })\n",
    "            append_jsonl(run_log, record)\n",
    "            failures.append(record)\n",
    "            records.append(record)\n",
    "\n",
    "    duration = time.perf_counter() - stage_start\n",
    "    return {\n",
    "        \"stage\": \"transcribe\",\n",
    "        \"duration_seconds\": round(duration, 3),\n",
    "        \"records\": records,\n",
    "        \"failures\": failures,\n",
    "        \"log_path\": str(run_log),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def read_style_reference_excerpt(paths: dict[str, Path], config: dict[str, Any], limit_chars: int = 1800) -> str:\n",
    "    rel_path = config.get(\"segmentation\", {}).get(\"style_reference\", \"00-dev-log/2026-02-09.md\")\n",
    "    style_path = paths[\"project_root\"] / rel_path\n",
    "    if not style_path.exists():\n",
    "        return \"\"\n",
    "    raw = style_path.read_text(encoding=\"utf-8\")\n",
    "    return raw[:limit_chars]\n",
    "\n",
    "\n",
    "def chunk_transcript_for_segmentation(\n",
    "    transcript_payload: dict[str, Any],\n",
    "    *,\n",
    "    window_seconds: float,\n",
    "    window_overlap_seconds: float,\n",
    ") -> list[dict[str, Any]]:\n",
    "    segments = transcript_payload.get(\"segments\", [])\n",
    "    if not segments:\n",
    "        return []\n",
    "\n",
    "    sorted_segments = sorted(segments, key=lambda item: float(item.get(\"start\", 0.0)))\n",
    "    chunks: list[dict[str, Any]] = []\n",
    "    cursor = float(sorted_segments[0].get(\"start\", 0.0))\n",
    "    final_end = float(sorted_segments[-1].get(\"end\", cursor))\n",
    "\n",
    "    while cursor <= final_end + 1e-6:\n",
    "        window_end = cursor + max(30.0, window_seconds)\n",
    "        in_window = [\n",
    "            seg\n",
    "            for seg in sorted_segments\n",
    "            if float(seg.get(\"end\", 0.0)) >= cursor and float(seg.get(\"start\", 0.0)) <= window_end\n",
    "        ]\n",
    "        if in_window:\n",
    "            chunks.append({\n",
    "                \"window_start\": cursor,\n",
    "                \"window_end\": window_end,\n",
    "                \"segments\": in_window,\n",
    "            })\n",
    "        step = max(20.0, window_seconds - window_overlap_seconds)\n",
    "        cursor += step\n",
    "        if cursor > final_end and chunks:\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_segmentation_schema() -> dict[str, Any]:\n",
    "    return {\n",
    "        \"name\": \"chat_segments\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"segments\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"id\": {\"type\": \"string\"},\n",
    "                            \"start_time\": {\"type\": \"number\"},\n",
    "                            \"end_time\": {\"type\": \"number\"},\n",
    "                            \"speaker\": {\"type\": \"string\"},\n",
    "                            \"summary\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\"id\", \"start_time\", \"end_time\", \"speaker\", \"summary\"],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"segments\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _http_json_request(\n",
    "    *,\n",
    "    method: str,\n",
    "    url: str,\n",
    "    api_key: str,\n",
    "    payload: dict[str, Any] | None = None,\n",
    "    timeout_seconds: float = 10.0,\n",
    ") -> dict[str, Any]:\n",
    "    import urllib.error\n",
    "    import urllib.request\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "    }\n",
    "    data = None\n",
    "    if payload is not None:\n",
    "        data = json.dumps(payload).encode(\"utf-8\")\n",
    "\n",
    "    req = urllib.request.Request(url=url, data=data, headers=headers, method=method)\n",
    "    try:\n",
    "        with urllib.request.urlopen(req, timeout=timeout_seconds) as response:\n",
    "            raw = response.read().decode(\"utf-8\")\n",
    "            return json.loads(raw)\n",
    "    except urllib.error.HTTPError as exc:\n",
    "        body = exc.read().decode(\"utf-8\", errors=\"replace\")\n",
    "        raise RuntimeError(f\"http_error_{exc.code}: {body}\") from exc\n",
    "\n",
    "\n",
    "def local_model_smoke_check(config: dict[str, Any]) -> dict[str, Any]:\n",
    "    seg_cfg = config.get(\"segmentation\", {})\n",
    "    timeout_seconds = float(seg_cfg.get(\"smoke_check_timeout_seconds\", 5))\n",
    "    base_url = str(seg_cfg.get(\"base_url\", \"http://localhost:1234/v1\")).rstrip(\"/\")\n",
    "    api_key = str(seg_cfg.get(\"api_key\", \"lm-studio\"))\n",
    "\n",
    "    try:\n",
    "        payload = _http_json_request(\n",
    "            method=\"GET\",\n",
    "            url=f\"{base_url}/models\",\n",
    "            api_key=api_key,\n",
    "            timeout_seconds=timeout_seconds,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"reason\": f\"local_model_unreachable: {exc}\",\n",
    "        }\n",
    "\n",
    "    available = [item.get(\"id\") for item in payload.get(\"data\", []) if item.get(\"id\")]\n",
    "    requested = seg_cfg.get(\"model\")\n",
    "    if requested and requested not in available:\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"reason\": f\"requested_model_not_available: {requested}\",\n",
    "            \"available_models\": available,\n",
    "        }\n",
    "    return {\"ok\": True, \"available_models\": available}\n",
    "\n",
    "\n",
    "def llm_segment_chunk(\n",
    "    *,\n",
    "    chunk: dict[str, Any],\n",
    "    style_excerpt: str,\n",
    "    config: dict[str, Any],\n",
    ") -> list[dict[str, Any]]:\n",
    "    seg_cfg = config.get(\"segmentation\", {})\n",
    "    transcript_lines: list[str] = []\n",
    "    for seg in chunk[\"segments\"]:\n",
    "        transcript_lines.append(\n",
    "            f\"[{float(seg.get('start', 0.0)):.2f}-{float(seg.get('end', 0.0)):.2f}] {seg.get('speaker', 'UNKNOWN')}: {str(seg.get('text', '')).strip()}\"\n",
    "        )\n",
    "\n",
    "    max_words = int(seg_cfg.get(\"max_summary_words\", 80))\n",
    "    prompt = (\n",
    "        \"You segment stream transcript text into chat-style blocks. \"\n",
    "        \"Return JSON only. Keep summaries faithful to transcript content and avoid making up details. \"\n",
    "        \"Transcribe exactly what is visible in transcript content and do not continue a repeating pattern. \"\n",
    "        \"Keep chronology valid and include slight overlap between adjacent segments.\\n\\n\"\n",
    "        f\"Window: {chunk['window_start']:.2f}s to {chunk['window_end']:.2f}s\\n\"\n",
    "        \"Use hybrid boundaries: semantic or topic shifts, but stay within this time window.\\n\"\n",
    "        f\"Required fields: id, start_time, end_time, speaker, summary\\n\"\n",
    "        f\"Summary length: <= {max_words} words per segment.\\n\\n\"\n",
    "        \"Style reference excerpt (2026-02-09):\\n\"\n",
    "        f\"{style_excerpt or '(unavailable)'}\\n\\n\"\n",
    "        \"Transcript lines:\\n\"\n",
    "        + \"\\n\".join(transcript_lines)\n",
    "    )\n",
    "\n",
    "    body = {\n",
    "        \"model\": seg_cfg.get(\"model\"),\n",
    "        \"temperature\": float(seg_cfg.get(\"temperature\", 0.2)),\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise transcript segmenter.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": build_segmentation_schema(),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    base_url = str(seg_cfg.get(\"base_url\", \"http://localhost:1234/v1\")).rstrip(\"/\")\n",
    "    response = _http_json_request(\n",
    "        method=\"POST\",\n",
    "        url=f\"{base_url}/chat/completions\",\n",
    "        api_key=str(seg_cfg.get(\"api_key\", \"lm-studio\")),\n",
    "        payload=body,\n",
    "        timeout_seconds=120.0,\n",
    "    )\n",
    "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    payload = json.loads(content)\n",
    "    return payload.get(\"segments\", [])\n",
    "\n",
    "def normalize_and_validate_segments(\n",
    "    raw_segments: list[dict[str, Any]],\n",
    "    *,\n",
    "    minimum_overlap_seconds: float,\n",
    ") -> list[dict[str, Any]]:\n",
    "    normalized: list[dict[str, Any]] = []\n",
    "    for idx, item in enumerate(raw_segments, start=1):\n",
    "        if not {\"id\", \"start_time\", \"end_time\", \"speaker\", \"summary\"}.issubset(set(item.keys())):\n",
    "            raise ValueError(\"segment_missing_required_fields\")\n",
    "\n",
    "        start_time = float(item[\"start_time\"])\n",
    "        end_time = float(item[\"end_time\"])\n",
    "        if end_time <= start_time:\n",
    "            end_time = start_time + 0.5\n",
    "\n",
    "        normalized.append({\n",
    "            \"id\": f\"seg-{idx:04d}\",\n",
    "            \"start_time\": round(start_time, 3),\n",
    "            \"end_time\": round(end_time, 3),\n",
    "            \"speaker\": str(item[\"speaker\"]).strip() or \"UNKNOWN\",\n",
    "            \"summary\": str(item[\"summary\"]).strip(),\n",
    "        })\n",
    "\n",
    "    normalized.sort(key=lambda item: (item[\"start_time\"], item[\"end_time\"]))\n",
    "\n",
    "    for idx in range(1, len(normalized)):\n",
    "        previous = normalized[idx - 1]\n",
    "        current = normalized[idx]\n",
    "        if current[\"start_time\"] < previous[\"start_time\"]:\n",
    "            raise ValueError(\"segments_not_chronological\")\n",
    "\n",
    "        target_start_max = previous[\"end_time\"] - minimum_overlap_seconds\n",
    "        if current[\"start_time\"] > target_start_max:\n",
    "            current[\"start_time\"] = round(max(0.0, target_start_max), 3)\n",
    "\n",
    "        if current[\"end_time\"] <= current[\"start_time\"]:\n",
    "            current[\"end_time\"] = round(current[\"start_time\"] + 0.5, 3)\n",
    "\n",
    "    for idx, item in enumerate(normalized, start=1):\n",
    "        item[\"id\"] = f\"seg-{idx:04d}\"\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def run_segmentation_stage(\n",
    "    *,\n",
    "    transcription_records: list[dict[str, Any]],\n",
    "    paths: dict[str, Path],\n",
    "    config: dict[str, Any],\n",
    "    run_id: str,\n",
    ") -> dict[str, Any]:\n",
    "    stage_start = time.perf_counter()\n",
    "    run_log = paths[\"logs_dir\"] / f\"segmentation-{run_id}.jsonl\"\n",
    "\n",
    "    records: list[dict[str, Any]] = []\n",
    "    failures: list[dict[str, Any]] = []\n",
    "    style_excerpt = read_style_reference_excerpt(paths, config)\n",
    "\n",
    "    smoke = local_model_smoke_check(config)\n",
    "    if not smoke.get(\"ok\"):\n",
    "        return {\n",
    "            \"stage\": \"segment\",\n",
    "            \"duration_seconds\": round(time.perf_counter() - stage_start, 3),\n",
    "            \"records\": [],\n",
    "            \"failures\": [{\n",
    "                \"run_id\": run_id,\n",
    "                \"stage\": \"segment\",\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": smoke.get(\"reason\"),\n",
    "                \"available_models\": smoke.get(\"available_models\", []),\n",
    "            }],\n",
    "            \"log_path\": str(run_log),\n",
    "            \"smoke_check\": smoke,\n",
    "        }\n",
    "\n",
    "    seg_cfg = config.get(\"segmentation\", {})\n",
    "\n",
    "    for item in transcription_records:\n",
    "        if item.get(\"status\") not in {\"ok\", \"reused\"}:\n",
    "            continue\n",
    "\n",
    "        transcript_path = Path(item[\"transcript_path\"])\n",
    "        record: dict[str, Any] = {\n",
    "            \"run_id\": run_id,\n",
    "            \"timestamp\": now_iso(),\n",
    "            \"stage\": \"segment\",\n",
    "            \"transcript_path\": str(transcript_path),\n",
    "            \"status\": \"pending\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            transcript_payload = json.loads(transcript_path.read_text(encoding=\"utf-8\"))\n",
    "            chunks = chunk_transcript_for_segmentation(\n",
    "                transcript_payload,\n",
    "                window_seconds=float(seg_cfg.get(\"window_seconds\", 240.0)),\n",
    "                window_overlap_seconds=float(seg_cfg.get(\"window_overlap_seconds\", 20.0)),\n",
    "            )\n",
    "\n",
    "            raw_segments: list[dict[str, Any]] = []\n",
    "            if not chunks:\n",
    "                normalized: list[dict[str, Any]] = []\n",
    "            else:\n",
    "                for chunk in chunks:\n",
    "                    raw_segments.extend(\n",
    "                        llm_segment_chunk(\n",
    "                            chunk=chunk,\n",
    "                            style_excerpt=style_excerpt,\n",
    "                            config=config,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                normalized = normalize_and_validate_segments(\n",
    "                    raw_segments,\n",
    "                    minimum_overlap_seconds=float(seg_cfg.get(\"minimum_overlap_seconds\", 1.5)),\n",
    "                )\n",
    "\n",
    "            record.update({\n",
    "                \"status\": \"ok\",\n",
    "                \"segment_count\": len(normalized),\n",
    "                \"segments\": normalized,\n",
    "            })\n",
    "            append_jsonl(run_log, record)\n",
    "            records.append(record)\n",
    "        except Exception as exc:\n",
    "            record.update({\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(exc),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "            })\n",
    "            append_jsonl(run_log, record)\n",
    "            failures.append(record)\n",
    "            records.append(record)\n",
    "\n",
    "    duration = time.perf_counter() - stage_start\n",
    "    return {\n",
    "        \"stage\": \"segment\",\n",
    "        \"duration_seconds\": round(duration, 3),\n",
    "        \"records\": records,\n",
    "        \"failures\": failures,\n",
    "        \"log_path\": str(run_log),\n",
    "        \"smoke_check\": smoke,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def segments_output_paths(paths: dict[str, Path]) -> dict[str, Path]:\n",
    "    segments_dir = paths[\"data_root\"] / \"segments\"\n",
    "    segments_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return {\n",
    "        \"segments_dir\": segments_dir,\n",
    "        \"json_path\": segments_dir / \"segments.json\",\n",
    "        \"markdown_path\": segments_dir / \"segments.md\",\n",
    "    }\n",
    "\n",
    "\n",
    "def render_segments_markdown(\n",
    "    *,\n",
    "    segments: list[dict[str, Any]],\n",
    "    transcript_path: str,\n",
    "    run_id: str,\n",
    ") -> str:\n",
    "    lines = [\n",
    "        \"# Chat-Style Segments\",\n",
    "        \"\",\n",
    "        f\"- run_id: {run_id}\",\n",
    "        f\"- transcript_path: {transcript_path}\",\n",
    "        f\"- segment_count: {len(segments)}\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    for segment in segments:\n",
    "        lines.extend([\n",
    "            f\"## {segment['id']}\",\n",
    "            f\"- start_time: {segment['start_time']}\",\n",
    "            f\"- end_time: {segment['end_time']}\",\n",
    "            f\"- speaker: {segment['speaker']}\",\n",
    "            f\"- summary: {segment['summary']}\",\n",
    "            \"\",\n",
    "        ])\n",
    "\n",
    "    return \"\\n\".join(lines).strip() + \"\\n\"\n",
    "\n",
    "\n",
    "def export_segments_pair(\n",
    "    *,\n",
    "    segments: list[dict[str, Any]],\n",
    "    transcript_path: str,\n",
    "    run_id: str,\n",
    "    paths: dict[str, Path],\n",
    ") -> dict[str, str]:\n",
    "    outputs = segments_output_paths(paths)\n",
    "    payload = {\n",
    "        \"schema_version\": \"1.0\",\n",
    "        \"run_id\": run_id,\n",
    "        \"transcript_path\": transcript_path,\n",
    "        \"segment_count\": len(segments),\n",
    "        \"segments\": segments,\n",
    "    }\n",
    "    outputs[\"json_path\"].write_text(json.dumps(payload, indent=2, ensure_ascii=True), encoding=\"utf-8\")\n",
    "\n",
    "    markdown_text = render_segments_markdown(\n",
    "        segments=segments,\n",
    "        transcript_path=transcript_path,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "    outputs[\"markdown_path\"].write_text(markdown_text, encoding=\"utf-8\")\n",
    "\n",
    "    return {\n",
    "        \"segments_dir\": str(outputs[\"segments_dir\"]),\n",
    "        \"segments_json\": str(outputs[\"json_path\"]),\n",
    "        \"segments_markdown\": str(outputs[\"markdown_path\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_segments_from_json(json_path: Path) -> list[dict[str, Any]]:\n",
    "    payload = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(payload, dict) and \"segments\" in payload:\n",
    "        return payload[\"segments\"]\n",
    "    if isinstance(payload, list):\n",
    "        return payload\n",
    "    raise ValueError(\"segments_json_invalid_shape\")\n",
    "\n",
    "\n",
    "def load_segments_from_markdown(markdown_path: Path) -> list[dict[str, Any]]:\n",
    "    import re\n",
    "\n",
    "    text = markdown_path.read_text(encoding=\"utf-8\")\n",
    "    pattern = re.compile(\n",
    "        r\"## (?P<id>.+?)\\n\"\n",
    "        r\"- start_time: (?P<start>.+?)\\n\"\n",
    "        r\"- end_time: (?P<end>.+?)\\n\"\n",
    "        r\"- speaker: (?P<speaker>.+?)\\n\"\n",
    "        r\"- summary: (?P<summary>.+?)(?:\\n\\n|\\Z)\",\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    segments: list[dict[str, Any]] = []\n",
    "    for match in pattern.finditer(text):\n",
    "        segments.append({\n",
    "            \"id\": match.group(\"id\").strip(),\n",
    "            \"start_time\": float(match.group(\"start\").strip()),\n",
    "            \"end_time\": float(match.group(\"end\").strip()),\n",
    "            \"speaker\": match.group(\"speaker\").strip(),\n",
    "            \"summary\": match.group(\"summary\").strip(),\n",
    "        })\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "def verify_segment_reload(json_path: Path, markdown_path: Path) -> dict[str, Any]:\n",
    "    json_segments = load_segments_from_json(json_path)\n",
    "    markdown_segments = load_segments_from_markdown(markdown_path)\n",
    "\n",
    "    json_ids = [segment[\"id\"] for segment in json_segments]\n",
    "    markdown_ids = [segment[\"id\"] for segment in markdown_segments]\n",
    "    return {\n",
    "        \"json_count\": len(json_segments),\n",
    "        \"markdown_count\": len(markdown_segments),\n",
    "        \"id_match\": json_ids == markdown_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_segment_export_stage(\n",
    "    *,\n",
    "    segmentation_records: list[dict[str, Any]],\n",
    "    paths: dict[str, Path],\n",
    "    run_id: str,\n",
    ") -> dict[str, Any]:\n",
    "    successes = [record for record in segmentation_records if record.get(\"status\") == \"ok\"]\n",
    "    if not successes:\n",
    "        return {\n",
    "            \"status\": \"skipped\",\n",
    "            \"reason\": \"no_segment_records\",\n",
    "        }\n",
    "\n",
    "    primary = successes[0]\n",
    "    outputs = export_segments_pair(\n",
    "        segments=primary.get(\"segments\", []),\n",
    "        transcript_path=str(primary.get(\"transcript_path\", \"\")),\n",
    "        run_id=run_id,\n",
    "        paths=paths,\n",
    "    )\n",
    "\n",
    "    reload_check = verify_segment_reload(\n",
    "        json_path=Path(outputs[\"segments_json\"]),\n",
    "        markdown_path=Path(outputs[\"segments_markdown\"]),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        **outputs,\n",
    "        \"reload_check\": reload_check,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collect_transcript_records_for_segmentation(\n",
    "    *,\n",
    "    transcription: dict[str, Any],\n",
    "    paths: dict[str, Path],\n",
    "    config: dict[str, Any],\n",
    "    run_id: str,\n",
    ") -> list[dict[str, Any]]:\n",
    "    records: list[dict[str, Any]] = []\n",
    "    seen: set[str] = set()\n",
    "\n",
    "    for record in transcription.get(\"records\", []):\n",
    "        if record.get(\"status\") not in {\"ok\", \"reused\"}:\n",
    "            continue\n",
    "        transcript_path = str(record.get(\"transcript_path\", \"\"))\n",
    "        if transcript_path and transcript_path not in seen:\n",
    "            records.append(record)\n",
    "            seen.add(transcript_path)\n",
    "\n",
    "    allow_reuse = bool(config.get(\"pipeline\", {}).get(\"allow_transcript_checkpoint_reuse\", True))\n",
    "    if not allow_reuse:\n",
    "        return records\n",
    "\n",
    "    glob_pattern = str(config.get(\"pipeline\", {}).get(\"reuse_transcript_glob\", \"*.json\"))\n",
    "    for transcript_path in sorted(paths[\"transcripts_dir\"].glob(glob_pattern)):\n",
    "        key = str(transcript_path)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        records.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"timestamp\": now_iso(),\n",
    "            \"stage\": \"transcribe\",\n",
    "            \"status\": \"reused\",\n",
    "            \"transcript_path\": key,\n",
    "            \"resume_marker\": True,\n",
    "            \"reuse_source\": \"transcript_checkpoint\",\n",
    "        })\n",
    "        seen.add(key)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def build_segmentation_guidance(smoke_check: dict[str, Any]) -> str:\n",
    "    reason = smoke_check.get(\"reason\", \"unknown\")\n",
    "    available = smoke_check.get(\"available_models\", [])\n",
    "    available_text = \", \".join(available) if available else \"none\"\n",
    "    return (\n",
    "        \"Segmentation model smoke check failed before full batch segmentation. \"\n",
    "        f\"Reason: {reason}. Available models: {available_text}. \"\n",
    "        \"Start your local model server at http://localhost:1234/v1 and set CONFIG['segmentation']['model'] to an available model id.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def write_run_metadata(\n",
    "    *,\n",
    "    run_id: str,\n",
    "    paths: dict[str, Path],\n",
    "    config: dict[str, Any],\n",
    "    extraction: dict[str, Any],\n",
    "    transcription: dict[str, Any],\n",
    "    segmentation: dict[str, Any],\n",
    "    export: dict[str, Any],\n",
    ") -> Path:\n",
    "    records_ex = extraction.get(\"records\", [])\n",
    "    records_tx = transcription.get(\"records\", [])\n",
    "    records_seg = segmentation.get(\"records\", [])\n",
    "\n",
    "    run_payload = {\n",
    "        \"run_id\": run_id,\n",
    "        \"created_at\": now_iso(),\n",
    "        \"paths\": {k: str(v) for k, v in paths.items()},\n",
    "        \"config_snapshot\": config,\n",
    "        \"inputs\": {\n",
    "            \"total_discovered\": len({r.get(\"input_media\") for r in records_ex if r.get(\"input_media\")}),\n",
    "            \"processed_ok_or_reused\": sum(1 for r in records_ex if r.get(\"status\") in {\"ok\", \"reused\"}),\n",
    "            \"failed\": sum(1 for r in records_ex if r.get(\"status\") == \"failed\"),\n",
    "        },\n",
    "        \"stages\": {\n",
    "            \"extract\": {\n",
    "                \"duration_seconds\": extraction.get(\"duration_seconds\", 0.0),\n",
    "                \"log_path\": extraction.get(\"log_path\"),\n",
    "                \"failure_log_path\": extraction.get(\"failure_log_path\"),\n",
    "                \"reused_count\": sum(1 for r in records_ex if r.get(\"status\") == \"reused\"),\n",
    "                \"new_count\": sum(1 for r in records_ex if r.get(\"status\") == \"ok\"),\n",
    "                \"failed_count\": len(extraction.get(\"failures\", [])),\n",
    "            },\n",
    "            \"transcribe\": {\n",
    "                \"duration_seconds\": transcription.get(\"duration_seconds\", 0.0),\n",
    "                \"log_path\": transcription.get(\"log_path\"),\n",
    "                \"reused_count\": sum(1 for r in records_tx if r.get(\"status\") == \"reused\"),\n",
    "                \"new_count\": sum(1 for r in records_tx if r.get(\"status\") == \"ok\"),\n",
    "                \"failed_count\": len(transcription.get(\"failures\", [])),\n",
    "                \"fallback_reasons\": [\n",
    "                    r.get(\"diarization_fallback_reason\")\n",
    "                    for r in records_tx\n",
    "                    if r.get(\"diarization_fallback_reason\")\n",
    "                ],\n",
    "            },\n",
    "            \"segment\": {\n",
    "                \"duration_seconds\": segmentation.get(\"duration_seconds\", 0.0),\n",
    "                \"log_path\": segmentation.get(\"log_path\"),\n",
    "                \"failed_count\": len(segmentation.get(\"failures\", [])),\n",
    "                \"smoke_check\": segmentation.get(\"smoke_check\"),\n",
    "                \"new_count\": sum(1 for r in records_seg if r.get(\"status\") == \"ok\"),\n",
    "            },\n",
    "            \"export\": export,\n",
    "        },\n",
    "        \"artifacts\": {\n",
    "            \"audio\": [r.get(\"audio_path\") for r in records_ex if r.get(\"audio_path\")],\n",
    "            \"transcripts\": [r.get(\"transcript_path\") for r in records_tx if r.get(\"transcript_path\")],\n",
    "            \"segments_json\": export.get(\"segments_json\"),\n",
    "            \"segments_markdown\": export.get(\"segments_markdown\"),\n",
    "        },\n",
    "        \"resume_markers\": {\n",
    "            \"audio_reused\": sum(1 for r in records_ex if r.get(\"status\") == \"reused\"),\n",
    "            \"transcripts_reused\": sum(1 for r in records_tx if r.get(\"status\") == \"reused\"),\n",
    "        },\n",
    "        \"failures\": {\n",
    "            \"extract\": extraction.get(\"failures\", []),\n",
    "            \"transcribe\": transcription.get(\"failures\", []),\n",
    "            \"segment\": segmentation.get(\"failures\", []),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_path = paths[\"runs_dir\"] / f\"run-{run_id}.json\"\n",
    "    output_path.write_text(json.dumps(run_payload, indent=2, ensure_ascii=True), encoding=\"utf-8\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def run_pipeline(config: dict[str, Any]) -> dict[str, Any]:\n",
    "    run_id = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\") + \"-\" + uuid4().hex[:8]\n",
    "    paths = resolve_project_paths()\n",
    "    inputs = discover_inputs(config, paths)\n",
    "\n",
    "    extraction = run_extraction_stage(inputs=inputs, paths=paths, config=config, run_id=run_id)\n",
    "    transcription = run_transcription_stage(\n",
    "        extraction_records=extraction.get(\"records\", []),\n",
    "        paths=paths,\n",
    "        config=config,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "\n",
    "    transcript_records = collect_transcript_records_for_segmentation(\n",
    "        transcription=transcription,\n",
    "        paths=paths,\n",
    "        config=config,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "\n",
    "    smoke_check = local_model_smoke_check(config)\n",
    "    if smoke_check.get(\"ok\"):\n",
    "        segmentation = run_segmentation_stage(\n",
    "            transcription_records=transcript_records,\n",
    "            paths=paths,\n",
    "            config=config,\n",
    "            run_id=run_id,\n",
    "        )\n",
    "    else:\n",
    "        segmentation = {\n",
    "            \"stage\": \"segment\",\n",
    "            \"duration_seconds\": 0.0,\n",
    "            \"records\": [],\n",
    "            \"failures\": [{\n",
    "                \"run_id\": run_id,\n",
    "                \"stage\": \"segment\",\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": smoke_check.get(\"reason\"),\n",
    "                \"guidance\": build_segmentation_guidance(smoke_check),\n",
    "            }],\n",
    "            \"log_path\": str(paths[\"logs_dir\"] / f\"segmentation-{run_id}.jsonl\"),\n",
    "            \"smoke_check\": smoke_check,\n",
    "        }\n",
    "\n",
    "    export = run_segment_export_stage(\n",
    "        segmentation_records=segmentation.get(\"records\", []),\n",
    "        paths=paths,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "\n",
    "    run_meta_path = write_run_metadata(\n",
    "        run_id=run_id,\n",
    "        paths=paths,\n",
    "        config=config,\n",
    "        extraction=extraction,\n",
    "        transcription=transcription,\n",
    "        segmentation=segmentation,\n",
    "        export=export,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"run_id\": run_id,\n",
    "        \"input_count\": len(inputs),\n",
    "        \"extract_failures\": len(extraction.get(\"failures\", [])),\n",
    "        \"transcribe_failures\": len(transcription.get(\"failures\", [])),\n",
    "        \"segment_failures\": len(segmentation.get(\"failures\", [])),\n",
    "        \"segment_smoke_check\": segmentation.get(\"smoke_check\"),\n",
    "        \"run_metadata\": str(run_meta_path),\n",
    "        \"audio_dir\": str(paths[\"audio_dir\"]),\n",
    "        \"transcripts_dir\": str(paths[\"transcripts_dir\"]),\n",
    "        \"segments_json\": export.get(\"segments_json\"),\n",
    "        \"segments_markdown\": export.get(\"segments_markdown\"),\n",
    "        \"export_status\": export.get(\"status\"),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = resolve_project_paths()\n",
    "{\n",
    "    key: as_project_relative(value, paths[\"project_root\"])\n",
    "    for key, value in paths.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "\n",
    "- Stage order: extraction -> transcription -> segmentation -> export.\n",
    "- Reuse defaults from Plan 01 are enabled (`force_reextract=False`, `force_retranscribe=False`).\n",
    "- If local model smoke check fails, the run exits segmentation early with guidance and preserves prior artifacts.\n",
    "- Segments export writes `segments/segments.json` and `segments/segments.md` in the same run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end orchestration example\n",
    "RUN_CONFIG = dict(CONFIG)\n",
    "RUN_CONFIG[\"input_mode\"] = \"single\"\n",
    "RUN_CONFIG[\"force_reextract\"] = False\n",
    "RUN_CONFIG[\"force_retranscribe\"] = False\n",
    "# RUN_RESULT = run_pipeline(RUN_CONFIG)\n",
    "# RUN_RESULT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
